# Training (Python)

This folder contains Python-side training/inference for the hybrid setup:

- Node.js (`src/`) runs live Mineflayer bot + Gemini
- Python (`Training/python/`) trains and serves policy models

## 1) Install Python deps

```bash
cd Training/python
pip install -r requirements.txt
```

## 2) Train a baseline model

Use the JSONL data generated by the Node recorder under `Training/datasets`:

Optional (recommended): clean observer datasets before training:

```bash
python clean_dataset.py --input ../datasets/state-action-YYYY-MM-DD.jsonl
```

This creates `state-action-YYYY-MM-DD.clean.jsonl` and:
- keeps observer rows (`action.source=observer-mode`) by default,
- removes noisy fields like `lastChatMessages`,
- drops absolute `state.position`/`state.observer.position` by default (use `--keep-absolute-position` to keep them),
- drops mixed rows where `state.position` does not match `state.observer.position`,
- downsamples repeated frames,
- remaps `OBSERVER_*` labels into existing action vocab (for current trainer compatibility).

Then train on the cleaned file:

Quick MLP sanity run (optional, 1-3 epochs):

```bash
python train.py --dataset ../datasets/state-action-YYYY-MM-DD.clean.jsonl --model-type mlp --epochs 3 --baseline-mlp
```

Default / recommended LSTM training:

```bash
python train.py --dataset ../datasets/state-action-YYYY-MM-DD.clean.jsonl --out-dir ../models --model-type lstm --sequence-length 8 --epochs 16 --batch-size 128 --lr 1e-3 --seed 42 --dropout 0.2 --hidden-size 128 --lstm-layers 1

Long-horizon LSTM (for multi-step behavior chains):

```bash
python train.py --dataset ../datasets/state-action-YYYY-MM-DD.jsonl --out-dir ../models --model-type lstm --sequence-length 64 --epochs 16 --batch-size 128 --lr 1e-3 --seed 42 --dropout 0.2 --hidden-size 256 --lstm-layers 2 --intent-loss-weight 0.8 --oversample-meaningful --intent-balanced-loss --min-feature-std 1e-3
```
```

Outputs:

- `Training/models/behavior_model.pt`
- `Training/models/behavior_model.meta.json`

Notes:
- Training now uses randomized split + class-weighted loss (helps with imbalanced action labels).
- Features now include velocity + delta-time, yaw/pitch orientation encoding, health/hunger, richer inventory buckets, top-3 nearby entity encoding (type + relative direction), and nearby-block spatial context from the 3Ã—3 neighborhood.
- Phase-1 normalization is enabled: train-time `mean/std` is saved and reused during inference.
- Absolute world position is removed from features to improve cross-map generalization.
- Phase-2 temporal modeling is available with `--model-type lstm` + `--sequence-length`.
- Policy training is now hybrid multi-head: discrete action classification (`ACTION_VOCAB`) + multi-intent prediction + continuous control targets (movement/aim/jump signals).
- Keep `ACTION_VOCAB` stable for current runtime compatibility; hybrid outputs are additive and can be consumed progressively.

## 3) Run local inference API

```bash
uvicorn serve_policy:app --host 127.0.0.1 --port 8765
```

Endpoints:

- `GET /health`
- `POST /predict` with payload:

```json
{
  "state": {
    "velocity": { "vx": 0, "vy": 0, "vz": 0 },
    "yaw": 0.0,
    "pitch": 0.0,
    "onGround": true,
    "inAir": false,
    "blockBelow": "grass_block",
    "blockFront": "air",
    "nearbyEntities": [],
    "inventory": []
  },
  "agent_id": "bot-1"
}
```

The response includes action label + confidence for Node integration.
For LSTM models, pass consistent `agent_id` so sequence memory is preserved across requests.
Hybrid models also return `intent_scores`, `active_intents`, and `continuous_control` for smoother non-atomic behavior execution.

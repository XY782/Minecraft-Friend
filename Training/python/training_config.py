from pathlib import Path


BASE_DIR = Path(__file__).resolve().parent


TRAINING_CONFIG = {
    'dataset': str((BASE_DIR / '../datasets/state-action-2026-02-19.jsonl').resolve()),
    'dataset_cache_enabled': True,
    'dataset_cache_dir': '',
    'out_dir': str((BASE_DIR / '../models').resolve()),
    'model_type': 'lstm',
    'sequence_length': 32,
    'sequence_length_strategy': 'adaptive',
    'sequence_length_min': 8,
    'sequence_length_max': 64,
    'min_train_windows': 512,
    'epochs': 16,
    'batch_size': 128,
    'eval_batch_size': 256,
    'lr': 1e-3,
    'weight_decay': 1e-4,
    'seed': 42,
    'device': 'auto',
    'dropout': 0.2,
    'hidden_size': 192,
    'lstm_layers': 2,
    'sequence_supervision': True,
    'lstm_bidirectional': False,
    'lstm_layer_norm': True,
    'normalize_features': True,
    'normalize_clip_value': 10.0,
    'normalize_preserve_binary': True,
    'normalize_log_scale': False,
    'class_weighted_loss': True,
    'class_weight_min': 0.3,
    'class_weight_max': 4.0,
    'class_weight_power': 0.5,
    'oversample_meaningful': True,
    'intent_balanced_loss': False,
    'explicit_intent_supervision': False,
    'intent_pos_weight_max': 6.0,
    'intent_loss_weight': 0.0,
    'control_loss_weight': 0.25,
    'min_feature_std': 1e-3,
    'grad_clip_norm': 1.0,
    'use_lr_scheduler': True,
    'lr_scheduler_factor': 0.5,
    'lr_scheduler_patience': 2,
    'lr_scheduler_min_lr': 1e-5,
    'early_stopping_patience': 5,
    'early_stopping_min_delta': 1e-6,
    'non_blocking_transfer': True,
}
